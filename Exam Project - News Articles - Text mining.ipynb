{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noticed-production",
   "metadata": {},
   "source": [
    "Author: Sandijs Vasilevskis, savas20@student.sdu.dk\n",
    "\n",
    "Date: 28-05-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quarterly-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-trance",
   "metadata": {},
   "source": [
    "# Downloading the dataset\n",
    "For the purposes of this project articles from 2019 until now are used. Note that the articles are sorted by the date they appeared on the front pages of news outlets so the article itself can be somewhat older and there will be article duplicates across time-stamps (one article could have stayed on front page for a bit).\n",
    "- Article data is described here: http://sciride.org/news.html\n",
    "- The processed articles are available here: https://news-mine.s3.eu-west-2.amazonaws.com/processed.tar.gz\n",
    "- With documentation here: http://sciride.org/news.html#datacontent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-aging",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "- Filtering articles by news outlet domain name, language and dates\n",
    "- Tagging articles related to COVID-19\n",
    "- Prepare training and test datasets\n",
    "- Tokenization of the article description field (from text to sentences to words), remove punctuation and English stopwords\n",
    "- Normalization of the tokenized text (stemming, lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "institutional-singapore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Files\\\\SDU Data Science [2020]\\\\SDU [2021]\\\\DM882 Text Mining\\\\Project\\\\release\\\\bbc.com\\\\per_day\\\\20190101.gz',\n",
       " 'C:\\\\Files\\\\SDU Data Science [2020]\\\\SDU [2021]\\\\DM882 Text Mining\\\\Project\\\\release\\\\bbc.com\\\\per_day\\\\20190102.gz',\n",
       " 'C:\\\\Files\\\\SDU Data Science [2020]\\\\SDU [2021]\\\\DM882 Text Mining\\\\Project\\\\release\\\\bbc.com\\\\per_day\\\\20190103.gz',\n",
       " 'C:\\\\Files\\\\SDU Data Science [2020]\\\\SDU [2021]\\\\DM882 Text Mining\\\\Project\\\\release\\\\bbc.com\\\\per_day\\\\20190104.gz',\n",
       " 'C:\\\\Files\\\\SDU Data Science [2020]\\\\SDU [2021]\\\\DM882 Text Mining\\\\Project\\\\release\\\\bbc.com\\\\per_day\\\\20190105.gz']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_articles(domain_name='bbc.com', date_from='2019-01-01', date_to='2021-01-01'):\n",
    "    \"\"\"This function retrieves the list of file paths from mayor online news outlets dataset.\n",
    "    These processed articles can be downloaded here: \n",
    "        https://news-mine.s3.eu-west-2.amazonaws.com/processed.tar.gz\n",
    "    N.B.:\n",
    "        This function can be used assuming that the compressed file from the link above is downloaded and\n",
    "        extracted under the 'release' folder and ontains all the news domain subfolders. \n",
    "        Otherwise the file path in function glob.glob() can be altered.\n",
    "    Args:   \n",
    "        domain_name (str): Subfolder name of the specific online news outlet i.e. 'bbc.com'\n",
    "        date_from (str): Start date string i.e.'2019-01-01'\n",
    "        date_to (str): End date string i.e.'2021-01-01'\n",
    "    Returns:\n",
    "        file_paths (list): File paths from the selected news article domain and corresponding time period.\n",
    "    \"\"\"\n",
    "    date_from = datetime.strptime(date_from, \"%Y-%m-%d\")\n",
    "    date_to = datetime.strptime(date_to, \"%Y-%m-%d\")\n",
    "    \n",
    "    file_paths = []\n",
    "    for file_name in glob.glob(f'release\\\\{domain_name}\\\\per_day\\\\*.gz'):\n",
    "        dt_string = os.path.basename(file_name).split('.')[0]\n",
    "        dt_object = datetime.strptime(dt_string, \"%Y%m%d\")\n",
    "    \n",
    "        if date_to >= dt_object >= date_from:            \n",
    "            file_path = os.path.join(os.getcwd(), file_name)\n",
    "            file_paths.append(file_path)\n",
    "            \n",
    "    return file_paths   \n",
    "      \n",
    "print(filter_articles()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-mapping",
   "metadata": {},
   "source": [
    "### Combine all filepaths from available news outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "civic-wound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55160\n",
      "['C:\\\\Files\\\\SDU Data Science [2020]\\\\SDU [2021]\\\\DM882 Text Mining\\\\Project\\\\release\\\\9news.com.au\\\\per_day\\\\20190101.gz', 'C:\\\\Files\\\\SDU Data Science [2020]\\\\SDU [2021]\\\\DM882 Text Mining\\\\Project\\\\release\\\\9news.com.au\\\\per_day\\\\20190102.gz', 'C:\\\\Files\\\\SDU Data Science [2020]\\\\SDU [2021]\\\\DM882 Text Mining\\\\Project\\\\release\\\\9news.com.au\\\\per_day\\\\20190103.gz', 'C:\\\\Files\\\\SDU Data Science [2020]\\\\SDU [2021]\\\\DM882 Text Mining\\\\Project\\\\release\\\\9news.com.au\\\\per_day\\\\20190104.gz', 'C:\\\\Files\\\\SDU Data Science [2020]\\\\SDU [2021]\\\\DM882 Text Mining\\\\Project\\\\release\\\\9news.com.au\\\\per_day\\\\20190105.gz']\n"
     ]
    }
   ],
   "source": [
    "# Read .json file with domain names and outlet language\n",
    "file = open('available_outlets.json', 'r', encoding='utf-8')\n",
    "outlets = json.loads(file.read())\n",
    "\n",
    "# Filter news outlets written in English only\n",
    "outlets_en = {}\n",
    "for k, v in outlets.items():    \n",
    "    if v['lng'][0] == 'en':        \n",
    "        outlets_en[k] = {'country': v['country']}\n",
    "        \n",
    "# List of domain names to iterate over\n",
    "domain_names = list(outlets_en.keys())\n",
    "\n",
    "# Collect all file paths (2019 - 2020)\n",
    "file_paths = []\n",
    "for domain_name in domain_names:    \n",
    "    file_paths.extend(filter_articles(domain_name, date_from='2019-01-01', date_to='2021-01-01'))\n",
    "    \n",
    "print(len(file_paths))\n",
    "print(file_paths[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-voluntary",
   "metadata": {},
   "source": [
    "### Tagging articles related to COVID-19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "genuine-sydney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tag_covid19(text):\n",
    "    \"\"\"This function uses regular expressions to identify if the text is COVID-19 related or not.\n",
    "     \"\"\"\n",
    "    # Note that '-' is replaced with ' ' to shorten the synonym list\n",
    "    text = text.replace('-', ' ')\n",
    "    \n",
    "    # COVID-19 related keywords\n",
    "    covid19_synonyms_list = ['covid',\n",
    "                             r'covid(.*?)19',                             \n",
    "                             'coronavirus disease 19',\n",
    "                             r'severe acute respiratory syndrome coronavirus(.*?)2',                             \n",
    "                             r'sars(.*?)cov(.*?)2',                          \n",
    "                             r'2019(.*?)n(.*?)cov',                     \n",
    "                             r'n(.*?)cov(.*?)2019',\n",
    "                             'coronavirus',\n",
    "                             'coronavirus 2019',\n",
    "                             'wuhan pneumonia',\n",
    "                             'wuhan virus',\n",
    "                             'wuhan coronavirus',\n",
    "                             r'coronavirus(.*?)2',\n",
    "                            'lockdown', 'quarantine', 'pandemic'\n",
    "                            ]\n",
    "    # ignore casing\n",
    "    if re.compile('|'.join(covid19_synonyms_list), re.IGNORECASE).search(text):            \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "tag_covid19('This text is about covid-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "strategic-spouse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "      <th>is_covid_tag</th>\n",
       "      <th>is_covid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cd06a6b02378991447309a60ed27bc9c</td>\n",
       "      <td>US election 2020: The young people struggling ...</td>\n",
       "      <td>How the pandemic is changing the economic pros...</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e0ddb2d43e898d0cf212baef9963618f</td>\n",
       "      <td>BBC - Travel - The private language of Venice</td>\n",
       "      <td>Although many travellers assume that the “auth...</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6f0b26eaa2adf75b51edb1eb8fa0bbb4</td>\n",
       "      <td>The African lake with explosive power - BBC Fu...</td>\n",
       "      <td>In central Africa is a deep lake that has a da...</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2c5da074e2b70b2e9e1f6e7b0948cb97</td>\n",
       "      <td>Said Benrahma: West Ham sign Algeria winger fr...</td>\n",
       "      <td>West Ham sign Algeria winger Said Benrahma fro...</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>336121e33d92e3fddeb2dad94d20098c</td>\n",
       "      <td>Salisbury Novichok-poisoned officer Nick Baile...</td>\n",
       "      <td>Det Sgt Nick Bailey \"had to admit defeat\" afte...</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         article_id  \\\n",
       "0  cd06a6b02378991447309a60ed27bc9c   \n",
       "1  e0ddb2d43e898d0cf212baef9963618f   \n",
       "2  6f0b26eaa2adf75b51edb1eb8fa0bbb4   \n",
       "3  2c5da074e2b70b2e9e1f6e7b0948cb97   \n",
       "4  336121e33d92e3fddeb2dad94d20098c   \n",
       "\n",
       "                                               title  \\\n",
       "0  US election 2020: The young people struggling ...   \n",
       "1      BBC - Travel - The private language of Venice   \n",
       "2  The African lake with explosive power - BBC Fu...   \n",
       "3  Said Benrahma: West Ham sign Algeria winger fr...   \n",
       "4  Salisbury Novichok-poisoned officer Nick Baile...   \n",
       "\n",
       "                                         description       date  is_covid_tag  \\\n",
       "0  How the pandemic is changing the economic pros... 2020-10-17          True   \n",
       "1  Although many travellers assume that the “auth... 2020-10-17         False   \n",
       "2  In central Africa is a deep lake that has a da... 2020-10-17         False   \n",
       "3  West Ham sign Algeria winger Said Benrahma fro... 2020-10-17         False   \n",
       "4  Det Sgt Nick Bailey \"had to admit defeat\" afte... 2020-10-17         False   \n",
       "\n",
       "   is_covid  \n",
       "0      True  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_article_data(file_path):\n",
    "    \"\"\"This function reads the relevant information zipped article files from the file path.\n",
    "    \"\"\"\n",
    "    dt_string = os.path.basename(file_path).split('.')[0]\n",
    "    dt_object = datetime.strptime(dt_string, \"%Y%m%d\")\n",
    "    \n",
    "    d = json.load(gzip.open(file_path))\n",
    "    \n",
    "    article_list = []\n",
    "    \n",
    "    for article_id, article_dict in d.items():        \n",
    "        dct = {'article_id': article_id, \n",
    "               'title': article_dict['title'], \n",
    "               'description': article_dict['description'], \n",
    "               'date': dt_object}\n",
    "        dct['is_covid_tag'] = any([tag_covid19(article_dict['description']), tag_covid19(article_dict['title'])])\n",
    "        dct['is_covid'] = article_dict['is_covid']\n",
    "        article_list.append(dct)\n",
    "        \n",
    "    # Remove duplicated articles    \n",
    "    df = pd.DataFrame(article_list).drop_duplicates()\n",
    "    return df\n",
    "    \n",
    "df = extract_article_data(filter_articles()[-1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ordered-exclusive",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "      <th>is_covid_tag</th>\n",
       "      <th>is_covid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>8d09044ee04e6c92f89a97a18315274a</td>\n",
       "      <td>China's mystery SARS-like virus spreads to Japan</td>\n",
       "      <td>Fears are mounting across Asia over the cross-...</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>7cd84a0cba91e97286902ccea4107f4c</td>\n",
       "      <td>China news: Efforts stepped up to contain coro...</td>\n",
       "      <td>China will step up efforts to contain the coro...</td>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>d053a48177e970959342b3e29be8660a</td>\n",
       "      <td>Brisbane man tested for mystery virus - 9News</td>\n",
       "      <td>The chief medical officer says the risk posed ...</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64c4cdc3bcf1e872451a622d51a77860</td>\n",
       "      <td>New strain of deadly China coronavirus 'can be...</td>\n",
       "      <td>Authorities in multiple countries, including C...</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7cd84a0cba91e97286902ccea4107f4c</td>\n",
       "      <td>China news: Efforts stepped up to contain coro...</td>\n",
       "      <td>China will step up efforts to contain the coro...</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>50b85a555631589517863708b0ec7247</td>\n",
       "      <td>Coronavirus in Ireland - All household visits ...</td>\n",
       "      <td>ALL household visits have been banned in a bid...</td>\n",
       "      <td>2020-10-15</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>c977a29a7b78c851a499e102128bcedb</td>\n",
       "      <td>Khloe Kardashian mocked for boasting about loo...</td>\n",
       "      <td>KHLOE Kardashian has been mocked for boasting ...</td>\n",
       "      <td>2020-10-15</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>5782e4fe7474d84dfb13a9a39c7b43e5</td>\n",
       "      <td>Coronavirus in Ireland - Taoiseach Micheal Mar...</td>\n",
       "      <td>DONEGAL, Cavan and Monaghan will be raised fro...</td>\n",
       "      <td>2020-10-15</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>6078d4ab83a04d942caef10152bfd0b8</td>\n",
       "      <td>Budget bonanza to see Govt try to build their ...</td>\n",
       "      <td>A CONSTRUCTION budget bonanza will see the ­Go...</td>\n",
       "      <td>2020-10-15</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>db83169dd5fef2810a8ed5a2b4393935</td>\n",
       "      <td>France puts Paris and eight other cities on 9p...</td>\n",
       "      <td>FRANCE will impose a 9pm curfew on Paris and e...</td>\n",
       "      <td>2020-10-15</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1090966 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           article_id  \\\n",
       "79   8d09044ee04e6c92f89a97a18315274a   \n",
       "53   7cd84a0cba91e97286902ccea4107f4c   \n",
       "10   d053a48177e970959342b3e29be8660a   \n",
       "25   64c4cdc3bcf1e872451a622d51a77860   \n",
       "49   7cd84a0cba91e97286902ccea4107f4c   \n",
       "..                                ...   \n",
       "218  50b85a555631589517863708b0ec7247   \n",
       "226  c977a29a7b78c851a499e102128bcedb   \n",
       "227  5782e4fe7474d84dfb13a9a39c7b43e5   \n",
       "253  6078d4ab83a04d942caef10152bfd0b8   \n",
       "261  db83169dd5fef2810a8ed5a2b4393935   \n",
       "\n",
       "                                                 title  \\\n",
       "79    China's mystery SARS-like virus spreads to Japan   \n",
       "53   China news: Efforts stepped up to contain coro...   \n",
       "10       Brisbane man tested for mystery virus - 9News   \n",
       "25   New strain of deadly China coronavirus 'can be...   \n",
       "49   China news: Efforts stepped up to contain coro...   \n",
       "..                                                 ...   \n",
       "218  Coronavirus in Ireland - All household visits ...   \n",
       "226  Khloe Kardashian mocked for boasting about loo...   \n",
       "227  Coronavirus in Ireland - Taoiseach Micheal Mar...   \n",
       "253  Budget bonanza to see Govt try to build their ...   \n",
       "261  France puts Paris and eight other cities on 9p...   \n",
       "\n",
       "                                           description       date  \\\n",
       "79   Fears are mounting across Asia over the cross-... 2020-01-16   \n",
       "53   China will step up efforts to contain the coro... 2020-01-19   \n",
       "10   The chief medical officer says the risk posed ... 2020-01-20   \n",
       "25   Authorities in multiple countries, including C... 2020-01-20   \n",
       "49   China will step up efforts to contain the coro... 2020-01-20   \n",
       "..                                                 ...        ...   \n",
       "218  ALL household visits have been banned in a bid... 2020-10-15   \n",
       "226  KHLOE Kardashian has been mocked for boasting ... 2020-10-15   \n",
       "227  DONEGAL, Cavan and Monaghan will be raised fro... 2020-10-15   \n",
       "253  A CONSTRUCTION budget bonanza will see the ­Go... 2020-10-15   \n",
       "261  FRANCE will impose a 9pm curfew on Paris and e... 2020-10-15   \n",
       "\n",
       "     is_covid_tag  is_covid  \n",
       "79           True      True  \n",
       "53           True      True  \n",
       "10           True      True  \n",
       "25           True      True  \n",
       "49           True      True  \n",
       "..            ...       ...  \n",
       "218          True      True  \n",
       "226          True      True  \n",
       "227          True      True  \n",
       "253          True      True  \n",
       "261          True      True  \n",
       "\n",
       "[1090966 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.is_covid==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-supervisor",
   "metadata": {},
   "source": [
    "### Combine data from all available news outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eligible-balance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2265.308422088623\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9519797 entries, 0 to 265\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Dtype         \n",
      "---  ------        -----         \n",
      " 0   article_id    object        \n",
      " 1   title         object        \n",
      " 2   description   object        \n",
      " 3   date          datetime64[ns]\n",
      " 4   is_covid_tag  bool          \n",
      " 5   is_covid      bool          \n",
      "dtypes: bool(2), datetime64[ns](1), object(3)\n",
      "memory usage: 381.3+ MB\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Combine all news articles into a DataFrame\n",
    "df_list = []\n",
    "for file_path in file_paths:      \n",
    "    try:\n",
    "        df_list.append(extract_article_data(file_path))\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "df = pd.concat(df_list)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time to extract data: {round((end - start)/ 60, 1)} minutes')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-substitute",
   "metadata": {},
   "source": [
    "## Training and Test Datasets\n",
    "Split the dataset - 80% for training and 20% for testing the classifier.\n",
    "\n",
    "### Training data\n",
    "Hint: picking articles with COVID-19 keyword in titles/descriptions would seed your training set of ‘positive’ articles whereas 2019 articles can be taken as\n",
    "the ‘negative’ set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bright-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting article description text\n",
    "X, y = df.description, df.is_covid\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "married-offset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (7615837,) (7615837,)\n",
      "Test (1903960,) (1903960,)\n"
     ]
    }
   ],
   "source": [
    "# Summarize\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "attempted-field",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    0.885345\n",
      "True     0.114655\n",
      "Name: is_covid, dtype: float64\n",
      "False    0.885622\n",
      "True     0.114378\n",
      "Name: is_covid, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the class balance after splitting data\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-extent",
   "metadata": {},
   "source": [
    "## Naive Bayes Text Classifier\n",
    "Creating a classifier based on news article title or description to determine whether the article is about COVID-19 or not. Estimating formally the performance of the classifier using accuracy $$(TN + TP)/(TN + TP + FN + FP)$$\n",
    "\n",
    "- Calculate Prior Probabilities for given class labels\n",
    "- Calculate Conditional Probabilities with each attribute for each class\n",
    "- Multiply same class Conditional Probability\n",
    "- Multiply Prior probability with Step 3 probability\n",
    "- Add Laplace smoothing to avoid zero probability for words that are not in training set\n",
    "- To determine class, see which class has higher posterior probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "descending-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNaiveBayes(object):\n",
    "    \"\"\"Implementation of Naive Bayes classifier for text binary classification.\"\"\"\n",
    " \n",
    "    def tokenize_text(self, text, remove_stopwords=True):\n",
    "        \"\"\"This function will tokenize text and remove stopwords.\n",
    "        \"\"\"\n",
    "        word_tokens = []\n",
    "        # Sentence tokenization\n",
    "        sentences_nltk = sent_tokenize(text)        \n",
    "        for sentence in sentences_nltk:\n",
    "            # Remove punctuation\n",
    "            sentence = sentence.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "            # Word tokenization        \n",
    "            tokens = [w for w in word_tokenize(sentence)]#if w.isalpha() # leave alphabetic characters only\n",
    "            # Remove stopwords\n",
    "            if remove_stopwords:\n",
    "                no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "                word_tokens.extend(no_stops)            \n",
    "            else:\n",
    "                word_tokens.extend(tokens)\n",
    "        return word_tokens\n",
    "    \n",
    "    def normalize_text(self, word_tokens, lemmatize_words=False):\n",
    "        \"\"\"This function uses Snowball stemmer or WordNet lemmatizer to normalize words.\n",
    "        \"\"\"\n",
    "        normalized_words = []   \n",
    "        # Lemmatize words\n",
    "        if lemmatize_words:        \n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            for word in word_tokens:            \n",
    "                normalized_words.append(wordnet_lemmatizer.lemmatize(word)) \n",
    "        # Stem words  \n",
    "        else:\n",
    "            snowball = SnowballStemmer('english')\n",
    "            for word in word_tokens:            \n",
    "                normalized_words.append(snowball.stem(word))           \n",
    "        return normalized_words\n",
    " \n",
    "    def get_word_counts(self, words):\n",
    "        \"\"\"This function returns a dictionary of word counts.\"\"\"\n",
    "        return dict(Counter(words))\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"This function calculates log class prior probabilities from the training data.\"\"\"\n",
    "        self.num_articles = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        n = len(X_train)\n",
    "        self.num_articles['is_target'] = sum(1 for label in y_train if label == True)\n",
    "        self.num_articles['not_target'] = sum(1 for label in y_train if label == False)\n",
    "        self.log_class_priors['is_target'] = np.log(self.num_articles['is_target'] / n)\n",
    "        self.log_class_priors['not_target'] = np.log(self.num_articles['not_target'] / n)\n",
    "        self.word_counts['is_target'] = {}\n",
    "        self.word_counts['not_target'] = {}\n",
    "\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            c = 'is_target' if y else 'not_target'\n",
    "            tokenized_text = self.tokenize_text(x)\n",
    "            normalized_text = self.normalize_text(tokenized_text)\n",
    "            counts = self.get_word_counts(normalized_text)\n",
    "\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"This function uses Naive Bayes calculation to return a list of predicted class labels.\"\"\"\n",
    "        result = []\n",
    "        for x in X_test:\n",
    "            tokenized_text = self.tokenize_text(x)\n",
    "            normalized_text = self.normalize_text(tokenized_text)\n",
    "            counts = self.get_word_counts(normalized_text)\n",
    "            is_target_score = 0\n",
    "            not_target_score = 0\n",
    "            \n",
    "            for word, _ in counts.items():\n",
    "                # Add Laplace smoothing for test words that are not present in training set\n",
    "                if word not in self.vocab: \n",
    "                    continue\n",
    "\n",
    "                # Log conditional probabilities for word given class                \n",
    "                log_w_given_is_target = np.log( (self.word_counts['is_target'].get(word, 0) + 1) / (self.num_articles['is_target'] + len(self.vocab)) )\n",
    "                log_w_given_not_target = np.log( (self.word_counts['not_target'].get(word, 0) + 1) / (self.num_articles['not_target'] + len(self.vocab)) )\n",
    "                # Sum all the log conditional probabilities\n",
    "                is_target_score += log_w_given_is_target\n",
    "                not_target_score += log_w_given_not_target\n",
    "\n",
    "            is_target_score += self.log_class_priors['is_target']\n",
    "            not_target_score += self.log_class_priors['not_target']\n",
    "\n",
    "            if is_target_score > not_target_score:\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "        return result\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        \"\"\"This function return accuracy score of the test data.\"\"\"\n",
    "        y_pred = self.predict(X_test)\n",
    "        return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "theoretical-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "CovidNB = TextNaiveBayes()\n",
    "CovidNB.fit(X_train[:100], y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "antique-database",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred = CovidNB.predict(X_test[:100])\n",
    "accuracy_score(y_test[:100], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "revolutionary-antibody",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score on test data\n",
    "CovidNB.score(X_test[:100], y_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "relevant-friday",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_target': -1.9661128563728327, 'not_target': -0.15082288973458366}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CovidNB.log_class_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fabulous-alexandria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1118"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CovidNB.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-uzbekistan",
   "metadata": {},
   "source": [
    "### Store the trained classifier model\n",
    "In order to reuse the model and not train it from the scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'Covid_text_classifier'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import pickle\n",
    "\n",
    "compressed_file = bz2.BZ2File(filename, 'wb')\n",
    "pickle.dump(model, compressed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(bz2.open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open(\"myfile.bz2\", \"wb\") as f:\n",
    "    # Write compressed data to file\n",
    "    unused = f.write(data)\n",
    "with bz2.open('Covid_text_classifier.bz2', 'rb') as f:    \n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-european",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Analysis\n",
    "\n",
    "Using the classifier, estimate the proportion of articles on COVID-19 in your dataset:\n",
    "- As proportion of all articles in 2020\n",
    "- As proportion of all articles in each month of 2020\n",
    "- As proportion of articles in an outlet (e.g. CNN) in 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-oasis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "every-jimmy",
   "metadata": {},
   "source": [
    "# Named Entity Recognisition\n",
    "Extract the Named Entities - in most cases these will correspond to names of people, places and organizations. You can create your own named entities extractor or use an off-the-shelf one. In either case you should be able to describe in your final report what the algorithm is doing, beyond simply stating the library you used.\n",
    "\n",
    "Based on the Named Entities analyze what are the most commonly mentioned Named Entities with respect to COVID-19.\n",
    "\n",
    "For example, 'en_core_web_sm' is a small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, vectors, syntax and named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "seventh-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "russian-census",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Larry Page', 'PERSON'),\n",
       " ('Google', 'ORG'),\n",
       " ('Apple', 'ORG'),\n",
       " ('covid-19', 'DATE')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading statistical models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Larry Page founded Google or Apple. Is it anything related to covid-19?\")\n",
    "\n",
    "# Text and label of named entity span\n",
    "[(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "limiting-polls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Larry Page,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "patent-greek",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Larry Page\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " founded \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". Is it anything related to \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    covid-19\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-ethics",
   "metadata": {},
   "source": [
    "# Using scikit-learn classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intense-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "flexible-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a dictionary of features and transforms documents to feature vectors and convert our text documents to a\n",
    "# matrix of token counts (CountVectorizer)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "# Transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "laughing-dictionary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "radical-broadway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.929617141149857"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text classifier pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Fitting our train data to the pipeline\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "text_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-arrest",
   "metadata": {},
   "source": [
    "### Random under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "universal-accused",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random under-sampling:\n",
      "False    13\n",
      "True     13\n",
      "Name: is_covid, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = df.is_covid.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df[df['is_covid'] == 0]\n",
    "df_class_1 = df[df['is_covid'] == 1]\n",
    "\n",
    "# Random under-sampling\n",
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_test_under.is_covid.value_counts())\n",
    "\n",
    "df_test_under.is_covid.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
